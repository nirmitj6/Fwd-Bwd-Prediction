# Fwd-Bwd-Prediction
In this, we traing GPT2 like architecture with ~175 M paramaeters on tinystories datasets. We train two model with identical hyperparmeter settings and architecture, but once that does next token prediction in thr forward direction and the other in the backward direction.

The code is coming soon!
